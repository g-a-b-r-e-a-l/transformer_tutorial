{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Transformer Model\n",
        "\n",
        "This is the code for a basic transformer model and generated sequences to avoid the need for a dataset. In the file 'Analysis' is an analysis of how the training loss changes as training goes on. This is a very simple exercise I used to better understand the Transformer architecture and how parameteres affect performance. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kk6yHPWATCrI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MaGYwc2uTRgz"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        # Ensure that the model dimension (d_model) is divisible by the number of heads\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        # Initialize dimensions\n",
        "        self.d_model = d_model # Model's dimension\n",
        "        self.num_heads = num_heads # Number of attention heads\n",
        "        self.d_k = d_model // num_heads # Dimension of each head's key, query, and value\n",
        "\n",
        "        # Linear layers for transforming inputs\n",
        "        self.W_q = nn.Linear(d_model, d_model) # Query transformation\n",
        "        self.W_k = nn.Linear(d_model, d_model) # Key transformation\n",
        "        self.W_v = nn.Linear(d_model, d_model) # Value transformation\n",
        "        self.W_o = nn.Linear(d_model, d_model) # Output transformation\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        # Calculate attention scores\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "        # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # Softmax is applied to obtain attention probabilities\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        # Multiply by values to obtain the final output\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # Reshape the input to have num_heads for multi-head attention\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        # Combine the multiple heads back to original shape\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        # Apply linear transformations and split heads\n",
        "        Q = self.split_heads(self.W_q(Q))\n",
        "        K = self.split_heads(self.W_k(K))\n",
        "        V = self.split_heads(self.W_v(V))\n",
        "\n",
        "        # Perform scaled dot-product attention\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        # Combine heads and apply output transformation\n",
        "        output = self.W_o(self.combine_heads(attn_output))\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_1eHhxdDTy7Q"
      },
      "outputs": [],
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MytcXCieVAKN"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xDSMVhK3XSaf"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VVjU3U1kZenl"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
        "        seq_length = tgt.size(1)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        output = self.fc(dec_output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KssZND2PmbvK"
      },
      "outputs": [],
      "source": [
        "# Define the PositionalEncoding class\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        # Create a matrix of positional encodings\n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        # Apply positional encoding formula\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Register the positional encodings as a buffer\n",
        "        pe = pe.unsqueeze(0)  # Add batch dimension\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add positional encodings to the input\n",
        "        return x + self.pe[:, :x.size(1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this block, I changed the vocabulary size, number of layers and heads, model dimensions and drop out to see what effect it had on the model performance. I have my basic analysis of how each parameter changes the learning rate in the 'analysis of results' file in the repository. \n",
        "\n",
        "The parameters have been manually optimised for low training time. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2aEHsdLlwFx",
        "outputId": "2dbe45e2-b119-4aa2-9323-b656bbc4ac3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of source data: torch.Size([64, 100])\n",
            "Shape of target data: torch.Size([64, 100])\n",
            "Example source sequence (numerical): tensor([ 5,  0,  2,  0,  8, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
            "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
            "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
            "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
            "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
            "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n",
            "Example target sequence (numerical): tensor([ 2,  6, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
            "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
            "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
            "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
            "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
            "        13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "\n",
        "# Assuming you have a 'Transformer' class and necessary helper modules (like PositionalEncoding) defined elsewhere.\n",
        "\n",
        "# Hyperparameters (these define the size and structure of your Transformer model)\n",
        "src_vocab_size = 50  # Size of the source vocabulary (tested values 5000, 1000, 500, 100, 50, 20, 15)\n",
        "tgt_vocab_size = 50  # Size of the target vocabulary (tested values 5000, 1000, 500, 100, 50, 20, 15)\n",
        "d_model = 256         # Dimensionality of the embedding layer and the internal representations of the Transformer\n",
        "                    #(tested values 1024, 512, 256, 64)\n",
        "num_heads = 8        # Number of attention heads in the multi-head attention mechanism (tested values 16, 8, 4, 2)\n",
        "num_layers = 2      # Number of encoder (and potentially decoder) layers in the Transformer (tested values 2, 4, 6, 8)\n",
        "d_ff = 2048         # Dimensionality of the feed-forward network within each Transformer layer \n",
        "max_seq_length = 100  # Maximum length of the input and output sequences that the model can handle\n",
        "dropout = 0.1       # Dropout probability for regularization (tested values 0.1, 0.1, 0.8)\n",
        "\n",
        "# Instantiate the Transformer model\n",
        "# This creates an instance of your Transformer class with the specified hyperparameters.\n",
        "# It initializes all the layers and parameters of the model.\n",
        "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
        "\n",
        "# --- Process to Create Arithmetic Expression Data ---\n",
        "batch_size = 64  # Number of arithmetic expressions to generate in this batch\n",
        "max_len = max_seq_length  # Using the defined max sequence length for padding\n",
        "\n",
        "def generate_arithmetic_expressions(num_samples, max_length):\n",
        "    \"\"\"\n",
        "    Generates a list of simple arithmetic expressions and their results.\n",
        "\n",
        "    Args:\n",
        "        num_samples (int): The number of expression-result pairs to generate.\n",
        "        max_length (int): The maximum length of the expression and result sequences (for padding).\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tuples, where each tuple contains:\n",
        "              - a list of tokens representing the arithmetic expression\n",
        "              - a list of tokens representing the result\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    operators = [\"+\", \"-\"]\n",
        "    digits = \"0123456789\"\n",
        "    for _ in range(num_samples):\n",
        "        num1 = random.randint(1, 9)\n",
        "        num2 = random.randint(1, 9)\n",
        "        op = random.choice(operators)\n",
        "        expression = f\"{num1} {op} {num2}\"\n",
        "        try:\n",
        "            result = str(eval(expression))\n",
        "            src_sequence = list(expression)  # Split the expression into characters (tokens)\n",
        "            tgt_sequence = list(result)      # Split the result into characters (tokens)\n",
        "\n",
        "            # Pad sequences to max_length with a '<pad>' token (assuming you'll handle this in your vocabulary)\n",
        "            src_sequence = src_sequence + ['<pad>'] * (max_length - len(src_sequence))\n",
        "            tgt_sequence = tgt_sequence + ['<pad>'] * (max_length - len(tgt_sequence))\n",
        "\n",
        "            data.append((src_sequence, tgt_sequence))\n",
        "        except:\n",
        "            continue  # Skip if there's an error in evaluation (shouldn't happen with simple +/-)\n",
        "    return data\n",
        "\n",
        "# Generate the arithmetic dataset\n",
        "arithmetic_data = generate_arithmetic_expressions(batch_size, max_len)\n",
        "\n",
        "# --- Process to Convert Text Data to Numerical Tensors ---\n",
        "# You'll need to have a vocabulary (mapping from tokens to numbers) for both the source and target.\n",
        "# Assuming you have 'src_vocab' and 'tgt_vocab' dictionaries created elsewhere.\n",
        "\n",
        "def numericalize_sequence(sequence, vocab, max_length):\n",
        "    \"\"\"\n",
        "    Converts a sequence of tokens into a list of numerical indices based on the vocabulary.\n",
        "    Pads the sequence to the maximum length.\n",
        "\n",
        "    Args:\n",
        "        sequence (list): A list of tokens (e.g., characters).\n",
        "        vocab (dict): A dictionary mapping tokens to their numerical indices.\n",
        "        max_length (int): The maximum length of the sequence.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: A 1D tensor of numerical indices.\n",
        "    \"\"\"\n",
        "    numericalized = [vocab.get(token, vocab.get('<unk>', 0)) for token in sequence] # Use <unk> if token not in vocab\n",
        "    padded = numericalized + [vocab.get('<pad>', 0)] * (max_length - len(numericalized))\n",
        "    return torch.tensor(padded)\n",
        "\n",
        "# Assuming you have built your source and target vocabularies ('src_vocab' and 'tgt_vocab')\n",
        "# based on the characters present in your arithmetic expressions and results.\n",
        "\n",
        "# Example vocabulary creation (you might have a more sophisticated way of doing this)\n",
        "all_chars = set()\n",
        "for src, tgt in arithmetic_data:\n",
        "    all_chars.update(src)\n",
        "    all_chars.update(tgt)\n",
        "all_chars.add('<pad>')\n",
        "all_chars = sorted(list(all_chars))\n",
        "char_to_index = {char: i for i, char in enumerate(all_chars)}\n",
        "src_vocab = char_to_index\n",
        "tgt_vocab = char_to_index # In this simple case, source and target vocab can be the same\n",
        "\n",
        "# Convert the generated arithmetic data into numerical tensors\n",
        "src_data = torch.stack([numericalize_sequence(item[0], src_vocab, max_len) for item in arithmetic_data])\n",
        "tgt_data = torch.stack([numericalize_sequence(item[1], tgt_vocab, max_len) for item in arithmetic_data])\n",
        "\n",
        "# Now 'src_data' and 'tgt_data' are PyTorch tensors containing the numerical representations\n",
        "# of your arithmetic expressions and their corresponding results, ready to be used for training.\n",
        "\n",
        "print(\"Shape of source data:\", src_data.shape)\n",
        "print(\"Shape of target data:\", tgt_data.shape)\n",
        "print(\"Example source sequence (numerical):\", src_data[0])\n",
        "print(\"Example target sequence (numerical):\", tgt_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "H0C1JrceZtNm",
        "outputId": "ffb99933-ad09-444e-af52-6f2e761e48e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 4.397154331207275\n",
            "Epoch: 2, Loss: 2.347404956817627\n",
            "Epoch: 3, Loss: 0.925678551197052\n",
            "Epoch: 4, Loss: 0.3321765661239624\n",
            "Epoch: 5, Loss: 0.15180957317352295\n",
            "Epoch: 6, Loss: 0.09386774152517319\n",
            "Epoch: 7, Loss: 0.07153841853141785\n",
            "Epoch: 8, Loss: 0.06143736466765404\n",
            "Epoch: 9, Loss: 0.0559372715651989\n",
            "Epoch: 10, Loss: 0.0526890754699707\n",
            "Epoch: 11, Loss: 0.05106659606099129\n",
            "Epoch: 12, Loss: 0.04930248484015465\n",
            "Epoch: 13, Loss: 0.04863334074616432\n",
            "Epoch: 14, Loss: 0.04794232174754143\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m output = transformer(src_data, tgt_data[:, :-\u001b[32m1\u001b[39m])\n\u001b[32m      9\u001b[39m loss = criterion(output.contiguous().view(-\u001b[32m1\u001b[39m, tgt_vocab_size), tgt_data[:, \u001b[32m1\u001b[39m:].contiguous().view(-\u001b[32m1\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m optimizer.step()\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gkg124\\.conda\\envs\\learning_transformers_env\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gkg124\\.conda\\envs\\learning_transformers_env\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gkg124\\.conda\\envs\\learning_transformers_env\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "transformer.train()\n",
        "\n",
        "for epoch in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    output = transformer(src_data, tgt_data[:, :-1])\n",
        "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgaThhKqhU58",
        "outputId": "26865306-0229-46df-88e7-b10a8ae08aa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: 1 - 7, Predicted: , True: -6\n",
            "Input: 2 + 6, Predicted: , True: 8\n",
            "Input: 5 + 9, Predicted: , True: 14\n",
            "Input: 9 + 9, Predicted: , True: 18\n",
            "Input: 9 + 4, Predicted: , True: 13\n",
            "Input: 2 - 5, Predicted: , True: -3\n",
            "Input: 4 - 2, Predicted: , True: 2\n",
            "Input: 9 + 6, Predicted: , True: 15\n",
            "Input: 7 + 7, Predicted: , True: 14\n",
            "Input: 4 - 3, Predicted: , True: 1\n",
            "Input: 5 - 6, Predicted: , True: -1\n",
            "Input: 2 + 9, Predicted: , True: 11\n",
            "Input: 2 + 6, Predicted: , True: 8\n",
            "Input: 9 - 3, Predicted: , True: 6\n",
            "Input: 4 + 6, Predicted: , True: 10\n",
            "Input: 7 - 2, Predicted: , True: 5\n",
            "Input: 9 - 2, Predicted: , True: 7\n",
            "Input: 4 + 4, Predicted: , True: 8\n",
            "Input: 6 - 9, Predicted: , True: -3\n",
            "Input: 9 + 8, Predicted: , True: 17\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Inference loop to generate the target sequence\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_seq_length):  \u001b[38;5;66;03m# Limit the generation length\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     output = \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_tensor\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pass both src and tgt\u001b[39;00m\n\u001b[32m     54\u001b[39m     \u001b[38;5;66;03m# Get the last predicted token\u001b[39;00m\n\u001b[32m     55\u001b[39m     predicted_index = torch.argmax(output[:, -\u001b[32m1\u001b[39m, :], dim=-\u001b[32m1\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gkg124\\.conda\\envs\\learning_transformers_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gkg124\\.conda\\envs\\learning_transformers_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, src, tgt)\u001b[39m\n\u001b[32m     31\u001b[39m dec_output = tgt_embedded\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dec_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.decoder_layers:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     dec_output = \u001b[43mdec_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m output = \u001b[38;5;28mself\u001b[39m.fc(dec_output)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gkg124\\.conda\\envs\\learning_transformers_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gkg124\\.conda\\envs\\learning_transformers_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mDecoderLayer.forward\u001b[39m\u001b[34m(self, x, enc_output, src_mask, tgt_mask)\u001b[39m\n\u001b[32m     14\u001b[39m x = \u001b[38;5;28mself\u001b[39m.norm1(x + \u001b[38;5;28mself\u001b[39m.dropout(attn_output))\n\u001b[32m     15\u001b[39m attn_output = \u001b[38;5;28mself\u001b[39m.cross_attn(x, enc_output, enc_output, src_mask)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m ff_output = \u001b[38;5;28mself\u001b[39m.feed_forward(x)\n\u001b[32m     18\u001b[39m x = \u001b[38;5;28mself\u001b[39m.norm3(x + \u001b[38;5;28mself\u001b[39m.dropout(ff_output))\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gkg124\\.conda\\envs\\learning_transformers_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1744\u001b[39m             tracing_state.pop_scope()\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1748\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1749\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Ensure your Transformer model is in evaluation mode\n",
        "transformer.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "transformer.to(device)\n",
        "\n",
        "# Reuse the data generation function to create a validation set\n",
        "def generate_arithmetic_expressions(num_samples=32, max_length=max_seq_length): # Smaller validation set\n",
        "    data = []\n",
        "    operators = [\"+\", \"-\"]\n",
        "    digits = \"0123456789\"\n",
        "    for _ in range(num_samples):\n",
        "        num1 = random.randint(1, 9)\n",
        "        num2 = random.randint(1, 9)\n",
        "        op = random.choice(operators)\n",
        "        expression = f\"{num1} {op} {num2}\"\n",
        "        try:\n",
        "            result = str(eval(expression))\n",
        "            src_sequence = list(expression)\n",
        "            tgt_sequence = list(result)\n",
        "            src_sequence = src_sequence + ['<pad>'] * (max_length - len(src_sequence))\n",
        "            tgt_sequence = tgt_sequence + ['<pad>'] * (max_length - len(tgt_sequence))\n",
        "            data.append((src_sequence, tgt_sequence))\n",
        "        except:\n",
        "            continue\n",
        "    return data\n",
        "\n",
        "# Reuse the numericalization function\n",
        "def numericalize_sequence(sequence, vocab, max_length):\n",
        "    numericalized = [vocab.get(token, vocab.get('<unk>', 0)) for token in sequence]\n",
        "    padded = numericalized + [vocab.get('<pad>', 0)] * (max_length - len(numericalized))\n",
        "    return torch.tensor(padded).unsqueeze(0).to(device) # Add batch dimension\n",
        "\n",
        "# Generate the validation dataset\n",
        "validation_data = generate_arithmetic_expressions(num_samples=32)\n",
        "\n",
        "# Evaluation loop\n",
        "correct_predictions = 0\n",
        "total_samples = len(validation_data)\n",
        "\n",
        "with torch.no_grad(): # Disable gradient calculation during evaluation\n",
        "    for src_text, true_tgt_text in validation_data:\n",
        "        src_tensor = numericalize_sequence(src_text, src_vocab, max_seq_length)\n",
        "\n",
        "        # **CORRECTION STARTS HERE**\n",
        "        # Create a target tensor for inference (start with a <start> token)\n",
        "        # Replace '<start>' with your actual start token if different\n",
        "        tgt_tensor = torch.tensor([tgt_vocab.get('<start>', 0)]).unsqueeze(0).to(device)\n",
        "\n",
        "        # Inference loop to generate the target sequence\n",
        "        for _ in range(max_seq_length):  # Limit the generation length\n",
        "            output = transformer(src_tensor, tgt_tensor)  # Pass both src and tgt\n",
        "            # Get the last predicted token\n",
        "            predicted_index = torch.argmax(output[:, -1, :], dim=-1)\n",
        "            # Append the predicted token to the target sequence\n",
        "            tgt_tensor = torch.cat([tgt_tensor, predicted_index.unsqueeze(0)], dim=1)\n",
        "            # Stop if the predicted token is the <end> token\n",
        "            # Replace '<end>' with your actual end token if different\n",
        "            if predicted_index.item() == tgt_vocab.get('<end>', 0):\n",
        "                break\n",
        "        # **CORRECTION ENDS HERE**\n",
        "\n",
        "        # Get the predicted sequence (argmax over the vocabulary dimension)\n",
        "        predicted_indices = torch.argmax(output, dim=-1).squeeze(0).cpu().numpy()\n",
        "\n",
        "        # Convert predicted indices back to tokens\n",
        "        index_to_tgt_char = {i: char for char, i in tgt_vocab.items()}\n",
        "        predicted_tokens = [index_to_tgt_char.get(idx, '<unk>') for idx in predicted_indices]\n",
        "\n",
        "        # Stop prediction at the first padding token\n",
        "        if '<pad>' in predicted_tokens:\n",
        "            predicted_tokens = predicted_tokens[:predicted_tokens.index('<pad>')]\n",
        "\n",
        "        # Stop true target at the first padding token\n",
        "        if '<pad>' in true_tgt_text:\n",
        "            true_tgt_text = true_tgt_text[:true_tgt_text.index('<pad>')]\n",
        "\n",
        "        predicted_result = \"\".join(predicted_tokens)\n",
        "        true_result = \"\".join(true_tgt_text)\n",
        "\n",
        "        print(f\"Input: {''.join(src_text).replace('<pad>', '')}, Predicted: {predicted_result}, True: {true_result}\")\n",
        "\n",
        "        if predicted_result == true_result:\n",
        "            correct_predictions += 1\n",
        "\n",
        "accuracy = correct_predictions / total_samples\n",
        "print(f\"\\nValidation Accuracy: {accuracy * 100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
